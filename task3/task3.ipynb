{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ff4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe0c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4dd39759-4669-462c-b13a-fb0bb3e9165d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 581ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4dd39759-4669-462c-b13a-fb0bb3e9165d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/10ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/29 00:24:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/04/29 00:24:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/04/29 00:24:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/04/29 00:24:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "23/04/29 00:24:59 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.14.0.jar added multiple times to distributed cache.\n",
      "23/04/29 00:24:59 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.8.0.jar added multiple times to distributed cache.\n",
      "23/04/29 00:24:59 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-2.3.4.jar added multiple times to distributed cache.\n",
      "23/04/29 00:24:59 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar added multiple times to distributed cache.\n",
      "23/04/29 00:25:06 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql.functions import udf, explode, col, when, lower\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from operator import add\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format('xml').options(rowTag='page').load('hdfs:/wiki-whole.xml')\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7718768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8111734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#write the udf and regrex function\n",
    "def extract_text(text):\n",
    "    try:\n",
    "        match = re.findall(r'\\[\\[[^\\[\\]]+\\]\\]',text)\n",
    "    except:\n",
    "        match = []\n",
    "    output = []\n",
    "    for link in match:\n",
    "        temp = link.split('|')[0].lower()\n",
    "        if (':' in temp) and ('Category:' not in temp):\n",
    "            continue\n",
    "        elif '#' in temp:\n",
    "            continue\n",
    "        else: output.append(temp)\n",
    "    return [re.sub(r'\\[|\\]','',a) for a in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e65098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "udf_extract_text = udf(lambda x:extract_text(x), ArrayType(StringType()))\n",
    "#select columns that we need\n",
    "df2 = df.select(col('title'), col('revision.text._VALUE').alias('links'))\n",
    "#explode the links column\n",
    "df2 = df2.withColumn('internal_links',explode(udf_extract_text(df2['links'])))\n",
    "#lower all the rows\n",
    "df2 = df2.select(lower(col('title')).alias('title'),col('internal_links').alias('internal_links'))\n",
    "#drop na rows\n",
    "df2 = df2.select(col('title'),col('internal_links')).na.drop()\n",
    "#sort rows in ascending order\n",
    "df2  = df2.select(col('title'),col('internal_links')).sort(['title','internal_links'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17faaff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- internal_links: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f28c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 00:26:12 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|      internal_links|\n",
      "+--------------------+--------------------+\n",
      "|\"hello, world!\" p...|                .deb|\n",
      "|\"hello, world!\" p...|3d computer graphics|\n",
      "|\"hello, world!\" p...|ada (programming ...|\n",
      "|\"hello, world!\" p...|            algol 60|\n",
      "|\"hello, world!\" p...|application progr...|\n",
      "|\"hello, world!\" p...|               ascii|\n",
      "|\"hello, world!\" p...|b (programming la...|\n",
      "|\"hello, world!\" p...|               basic|\n",
      "|\"hello, world!\" p...|                bcpl|\n",
      "|\"hello, world!\" p...|           bell labs|\n",
      "|\"hello, world!\" p...|     brian kernighan|\n",
      "|\"hello, world!\" p...|     brian kernighan|\n",
      "|\"hello, world!\" p...|c (programming la...|\n",
      "|\"hello, world!\" p...|c sharp (programm...|\n",
      "|\"hello, world!\" p...|                 c++|\n",
      "|\"hello, world!\" p...|                 c++|\n",
      "|\"hello, world!\" p...|         catchphrase|\n",
      "|\"hello, world!\" p...|               cobol|\n",
      "|\"hello, world!\" p...|complex programma...|\n",
      "|\"hello, world!\" p...|            computer|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a75ab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df2.select(col('title')).distinct().union(df2.select(col('internal_links')).distinct())\n",
    "# get all articles to generate initial ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd37ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 00:27:14 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/29 00:27:15 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "[Stage 3:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|               aruba|\n",
      "|                 art|\n",
      "|               anime|\n",
      "|antigua and barbu...|\n",
      "|  economy of armenia|\n",
      "|  ancient philosophy|\n",
      "|       alligatoridae|\n",
      "|albert of branden...|\n",
      "|american airlines...|\n",
      "|   american folklore|\n",
      "|            april 24|\n",
      "|         amblygonite|\n",
      "|         brassicales|\n",
      "|  batman (1989 film)|\n",
      "|            chordate|\n",
      "|cutaway (filmmaking)|\n",
      "|           actresses|\n",
      "|analysis of variance|\n",
      "| ara (constellation)|\n",
      "|astronomical year...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# articles.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a46e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 00:28:42 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "[Stage 8:===================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "ranks = articles.rdd.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f947ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627b4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "df2_rdd = df2.rdd.groupByKey().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7977a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cinema of germany', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d580>), ('cinema of poland', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d520>), ('cinema of the soviet union', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d5e0>), ('ciprofloxacin', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d640>), ('circle', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d670>), ('circuit noise level', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d6d0>), ('circuit restoration', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d730>), ('circular polarization', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d790>), ('circumference', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d7f0>), ('circumsision', <pyspark.resultiterable.ResultIterable object at 0x7ff87532d850>)]\n"
     ]
    }
   ],
   "source": [
    "# print(df2_rdd.collect()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d619ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contribution = df2_rdd.join(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c4001dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ezra abbot', (<pyspark.resultiterable.ResultIterable object at 0x7ff875f431c0>, 1.0)), ('ezra abbot', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcf460>, 1.0)), ('catalyst', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcf2b0>, 1.0)), ('catalyst', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcf280>, 1.0)), ('cognitive psychology', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcf1c0>, 1.0)), ('cognitive psychology', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcf1c0>, 1.0)), ('color blindness', (<pyspark.resultiterable.ResultIterable object at 0x7ff875bcfe80>, 1.0)), ('color blindness', (<pyspark.resultiterable.ResultIterable object at 0x7ff87532d190>, 1.0)), ('e. p. thompson on luddites', (<pyspark.resultiterable.ResultIterable object at 0x7ff87532d160>, 1.0)), ('list of female tennis players', (<pyspark.resultiterable.ResultIterable object at 0x7ff87532d460>, 1.0))]\n"
     ]
    }
   ],
   "source": [
    "# print(contribution.collect()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57a3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    # calculate contribution for each url in urls\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff9b5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "for iteration in range(iterations):\n",
    "    contribution = df2_rdd.join(ranks).flatMap(lambda x: computeContribs(\n",
    "            x[1][0], x[1][1]  # type: ignore[arg-type]\n",
    "        ))\n",
    "    # (article0, (neighbours, rank)) -> (article, contribution)\n",
    "    ranks = contribution.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d12940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ranks_df= ranks.toDF().withColumnRenamed(\"_1\",\"article\").withColumnRenamed(\"_2\",\"rank\").sort([\"rank\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adfbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:======================================================> (58 + 2) / 60]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|          article|              rank|\n",
      "+-----------------+------------------+\n",
      "|  catholic church|22.648388248183334|\n",
      "|           france|12.599248243352337|\n",
      "|    new york city|10.322981159646607|\n",
      "|         buddhism| 8.501049791006622|\n",
      "|            islam| 6.994866345599904|\n",
      "|       philosophy| 6.449267209157854|\n",
      "|      linguistics| 5.304698617381028|\n",
      "|     cryptography|  4.87365518769875|\n",
      "|   climate change| 4.671917261754584|\n",
      "|personal computer| 4.399111028276592|\n",
      "+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ranks_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee10edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ranks_df.limit(10).toPandas().to_csv('work/task3-whole.csv',header = False, index = False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d52923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
